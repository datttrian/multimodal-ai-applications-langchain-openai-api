{"cells":[{"cell_type":"markdown","id":"bbf7941b-6c0a-4b14-b1fc-c703e57e352b","metadata":{},"source":["# Building Multimodal AI Applications with LangChain & the OpenAI API "]},{"cell_type":"markdown","id":"333b6c1a-a3c4-4c3c-b5e1-145bd214f4e0","metadata":{},"source":["## Goals "]},{"cell_type":"markdown","id":"701b76fe-04db-405c-98f7-f0f5babd84b4","metadata":{},"source":["Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\n","\n","In this project, you'll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content."]},{"cell_type":"markdown","id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","metadata":{},"source":["- Understanding the building blocks of working with Multimodal AI projects\n","- Working with some of the fundamental concepts of LangChain  \n","- How to use the Whisper API to transcribe audio to text \n","- How to combine both LangChain and Whisper API to create ask questions of any YouTube video "]},{"cell_type":"markdown","id":"8231d2c6-275e-4399-b7cd-84e112831d08","metadata":{},"source":["## Before you begin"]},{"cell_type":"markdown","id":"785d7fac-edb2-482f-be2b-c63dc2882103","metadata":{},"source":["You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up."]},{"cell_type":"markdown","id":"a9274661-8d8c-4cc5-901e-5fc497866b89","metadata":{},"source":["## Task 0: Setup"]},{"cell_type":"markdown","id":"823598ac-fa77-4532-997d-2923d0017e90","metadata":{},"source":["The project requires several packages that need to be installed into Workspace.\n","\n","- `langchain` is a framework for developing generative AI applications.\n","- `yt_dlp` lets you download YouTube videos.\n","- `tiktoken` converts text into tokens.\n","- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text)."]},{"cell_type":"markdown","id":"c17ab340-c582-4ba7-ab33-5d582210f5c2","metadata":{},"source":["### Instructions\n","\n","Run the following code to install the packages."]},{"cell_type":"code","execution_count":null,"id":"e1ca41e3-2dfd-4b9a-b595-e5af720ca36a","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":10919,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1709640899353,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Lock OpenAI version to 0.27.1\n!pip install openai==0.27.1\n# Install langchain\n!pip install langchain==0.0.292","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[],"source":["# Lock OpenAI version to 0.27.1\n","!pip install openai==0.27.1\n","# Install langchain\n","!pip install langchain==0.0.292"]},{"cell_type":"code","execution_count":null,"id":"4eb141d6-8eaf-4fe2-b29a-e58765a02252","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":5044,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1695387659142,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Install yt_dlp\n!pip install yt_dlp==2023.7.6","outputsMetadata":{"0":{"height":447,"type":"stream"}}},"outputs":[],"source":["# Install yt_dlp\n","!pip install yt_dlp==2023.7.6"]},{"cell_type":"code","execution_count":null,"id":"3dce7fdd-9f8a-4517-a1bb-d03221dddc9e","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":2563,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1695387663053,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install tiktoken==0.5.1","outputsMetadata":{"0":{"height":447,"type":"stream"}}},"outputs":[],"source":["!pip install tiktoken==0.5.1"]},{"cell_type":"code","execution_count":null,"id":"6a7d041f-ea6c-470f-a3ca-1361f161021d","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":5036,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1695387673657,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install docarray==0.38.0","outputsMetadata":{"0":{"height":557,"type":"stream"}}},"outputs":[],"source":["!pip install docarray==0.38.0"]},{"cell_type":"markdown","id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","metadata":{},"source":["### Instructions"]},{"cell_type":"markdown","id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","metadata":{},"source":["## Task 1: Import The Required Libraries "]},{"cell_type":"markdown","id":"2cf847fd-f8f8-49f6-9b43-0eb098239072","metadata":{},"source":["For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. "]},{"cell_type":"markdown","id":"b1fcd794-b29c-4010-8be0-651a452b2044","metadata":{},"source":["Import the following packages.\n","\n","- Import `os` \n","- Import `openai`\n","- Import `yt_dlp` with the alias `youtube_dl`\n","- From the `yt_dlp` package, import `DowloadError`\n","- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`"]},{"cell_type":"code","execution_count":7,"id":"366f3234","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install --quiet openai yt_dlp docarray"]},{"cell_type":"code","execution_count":1,"id":"541cd9f5-0aaa-4374-8411-25bedecd8c84","metadata":{"executionCancelledAt":null,"executionTime":2566,"lastExecutedAt":1695381616329,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\n# Import the os package\nimport os  \n\n# Import the glob package\nimport glob\n\n# Import the openai package \nimport openai \n\n# Import the yt_dlp package as youtube_dl\nimport yt_dlp as youtube_dl \n\n# Import DownloadError from yt_dlp\nfrom yt_dlp import DownloadError \n\nimport docarray ","outputsMetadata":{"0":{"height":77,"type":"stream"}}},"outputs":[],"source":["# Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n","\n","# Import the os package\n","import os \n","\n","# Import the glob package\n","import glob\n","\n","# Import the openai package \n","import openai \n","\n","# Import the yt_dlp package as youtube_dl\n","import yt_dlp as youtube_dl\n","\n","# Import DownloadError from yt_dlp\n","from yt_dlp import DownloadError \n","\n","# Import DocArray \n","import docarray \n"]},{"cell_type":"markdown","id":"794e2ce2-ba13-446f-9ac7-2b5743f65a51","metadata":{},"source":["We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. "]},{"cell_type":"code","execution_count":2,"id":"7156b205-f844-4d9e-8867-449ff5840839","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1695381359221,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")"},"outputs":[],"source":["openai_api_key = os.getenv(\"OPENAI_API_KEY\")"]},{"cell_type":"markdown","id":"751b9539-cbf7-4d6e-9634-045345cf8a4a","metadata":{},"source":["## Task 2: Download the YouTube Video"]},{"cell_type":"markdown","id":"48abc459-48e5-4c7c-a795-daaf347ceef6","metadata":{},"source":["After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n","\n","We'll download a DataCamp tutorial about machine learning in Python.\n","\n","We will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n","\n","The `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n","\n","Lastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. "]},{"cell_type":"markdown","id":"8f2f2698-f768-4437-8e7f-c11327d3d4a7","metadata":{},"source":["Create the following: \n","- Two variables - `youtube_url` to store the Video URL and `output_dir` that will be the directory where the audio files will be saved. \n","- For this tutorial, we can set the `youtube_url` to the following `\"https://www.youtube.com/watch?v=aqzxYofJ_ck\"`and the `output_dir`to `files/audio/`. In the future, you can change these values. \n","- Use the `ydl_config` that is provided to you "]},{"cell_type":"code","execution_count":3,"id":"ffb3836d-7b1b-47db-9ccc-6910972dd045","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":13030,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1695381382849,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\n# Check if the output directory exists, if not create it\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Print a message indicating which video is being downloaded\n\nprint(f\"Downloading video from {youtube_url}\")\n\n\n# Attempt to download the video using the specified configuration\n# If a DownloadError occurs, attempt to download the video again\n\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\n\n","outputsMetadata":{"0":{"height":357,"type":"stream"},"1":{"height":137,"type":"stream"},"2":{"height":97,"type":"stream"},"3":{"height":37,"type":"stream"},"4":{"height":257,"type":"stream"},"5":{"height":77,"type":"stream"},"6":{"height":57,"type":"stream"},"7":{"height":57,"type":"stream"},"8":{"height":97,"type":"stream"},"9":{"height":77,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["[debug] Encodings: locale UTF-8, fs utf-8, pref UTF-8, out UTF-8 (No ANSI), error UTF-8 (No ANSI), screen UTF-8 (No ANSI)\n","[debug] yt-dlp version stable@2024.05.27 from yt-dlp/yt-dlp [12b248ce6] (pip) API\n","[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set(), 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Sec-Fetch-Mode': 'navigate'}}\n","[debug] Python 3.10.14 (CPython aarch64 64bit) - Linux-6.6.22-linuxkit-aarch64-with-glibc2.36 (OpenSSL 3.0.11 19 Sep 2023, glibc 2.36)\n","[debug] exe versions: ffmpeg 5.1.5-0 (setts), ffprobe 5.1.5-0\n","[debug] Optional libraries: Cryptodome-3.20.0, brotli-1.1.0, certifi-2024.06.02, mutagen-1.47.0, requests-2.32.3, sqlite3-3.40.1, urllib3-2.2.2, websockets-12.0\n","[debug] Proxy map: {}\n","[debug] Request Handlers: urllib, requests, websockets\n","[debug] Loaded 1820 extractors\n"]},{"name":"stdout","output_type":"stream","text":["Downloading video from https://www.youtube.com/watch?v=aqzxYofJ_ck\n","[youtube] Extracting URL: https://www.youtube.com/watch?v=aqzxYofJ_ck\n","[youtube] aqzxYofJ_ck: Downloading webpage\n","[youtube] aqzxYofJ_ck: Downloading ios player API JSON\n"]},{"name":"stderr","output_type":"stream","text":["[debug] Loading youtube-nsig.61b3b5e4 from cache\n","[debug] [youtube] Decrypted nsig Kx3in19ve-rMzz8NcqL => 9SLZ_COkAxBpag\n","[debug] Loading youtube-nsig.61b3b5e4 from cache\n","[debug] [youtube] Decrypted nsig HtTV1wbBC3ajvS0mb-u => rP8PWXmXNSxd-g\n"]},{"name":"stdout","output_type":"stream","text":["[youtube] aqzxYofJ_ck: Downloading m3u8 information\n"]},{"name":"stderr","output_type":"stream","text":["[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\n","[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n"]},{"name":"stdout","output_type":"stream","text":["[info] aqzxYofJ_ck: Downloading 1 format(s): 251\n"]},{"name":"stderr","output_type":"stream","text":["[debug] Invoking http downloader on \"https://rr3---sn-tt1e7nlz.googlevideo.com/videoplayback?expire=1719361179&ei=Owp7ZtOTHriulu8PrMWRiAk&ip=209.120.255.181&id=o-AAU5VohW9Gpl75GJBLyFnBddploq1G-t47fwoyjNBtgh&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=zw&mm=31%2C26&mn=sn-tt1e7nlz%2Csn-vgqsrnzy&ms=au%2Conr&mv=m&mvi=3&pl=24&initcwndbps=990000&bui=AbKP-1OKHcxixTmXAR7AJOfXc97Rh_bT9nzq3oVinDyRRbZX4SntvuKbuEqKPKW8lTOJnyPCOylMwd2N&spc=UWF9f_CgjMJVhZKjPAFHEmbPPqPpD_EMuOXjD9wNUx_kp9oUNvuwPD4pPaAP&vprv=1&svpuc=1&mime=audio%2Fwebm&ns=BE3qMbmScKa9lseiRD645DMQ&rqh=1&gir=yes&clen=10932652&dur=752.701&lmt=1654008313150389&mt=1719339187&fvip=3&keepalive=yes&c=WEB&sefc=1&txp=5318224&n=rP8PWXmXNSxd-g&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIgD5ON10_BkdVOqPi4gKvdf7UgflQUtl5IuyNxNtoz1J8CIQCJIfg9UQQUsaxKIdrpUtMLt8mYFxdS_SYDO8vdWIs6xQ%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=AHlkHjAwRQIgOPdpd9SIjtP1L2Ee_B2YtTBRZ33DhrTUJJkx22caOZwCIQCmF1tyqNPIHkxAGolpad7_0jBblrQ289pl3dEEM-lxBA%3D%3D\"\n"]},{"name":"stdout","output_type":"stream","text":["[download] files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm has already been downloaded\n","[download] 100% of   10.43MiB\n"]},{"name":"stderr","output_type":"stream","text":["[debug] ffmpeg command line: ffprobe -show_streams 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm'\n"]},{"name":"stdout","output_type":"stream","text":["[ExtractAudio] Destination: files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"]},{"name":"stderr","output_type":"stream","text":["[debug] ffmpeg command line: ffmpeg -y -loglevel repeat+info -i 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm' -vn -acodec libmp3lame -b:a 192.0k -movflags +faststart 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3'\n"]},{"name":"stdout","output_type":"stream","text":["Deleting original file files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm (pass -k to keep)\n"]}],"source":["# An example YouTube tutorial video\n","youtube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n","# Directory to store the downloaded video\n","output_dir = \"files/audio/\"\n","\n","# Config for youtube-dl\n","ydl_config = {\n","    \"format\": \"bestaudio/best\",\n","    \"postprocessors\": [\n","        {\n","            \"key\": \"FFmpegExtractAudio\",\n","            \"preferredcodec\": \"mp3\",\n","            \"preferredquality\": \"192\",\n","        }\n","    ],\n","    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n","    \"verbose\": True\n","}\n","\n","# Check if the output directory exists, if not create it\n","if not os.path.exists(output_dir): \n","    os.makedirs(output_dir)\n","\n","\n","# Print a message indicating which video is being downloaded\n","\n","print(f\"Downloading video from {youtube_url}\")\n","\n","\n","# Attempt to download the video using the specified configuration\n","# If a DownloadError occurs, attempt to download the video again\n","\n","try: \n","    with youtube_dl.YoutubeDL(ydl_config) as ydl: \n","        ydl.download([youtube_url])\n","except DownloadError: \n","    with youtube_dl.YoutubeDL(ydl_config) as ydl: \n","        ydl.download([youtube_url])\n","\n"]},{"cell_type":"markdown","id":"df9c586d-309a-411b-90a5-6e81fe85eda4","metadata":{},"source":["To find the audio files that we will use the `glob`module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. "]},{"cell_type":"markdown","id":"0fa69a42-7065-4c3f-8699-fe48908f11b1","metadata":{},"source":["Create the following: \n","- A variable called `audio_files`that uses the glob module to find all matching files with the `.mp3` file extension \n","- Select the first first file in the list and assign it to `audio_filename`\n","- To verify the filename, print `audio_filename` "]},{"cell_type":"code","execution_count":4,"id":"c3d0a34d-ade9-4314-bc7d-480f165b3992","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1695381382903,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Find the audio file in the output directory\n\n# Find all the audio files in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n\n# Select the first audio file in the list\naudio_filename = audio_files[0]\n\n# Print the name of the selected audio file\nprint(audio_filename)","outputsMetadata":{"0":{"height":56,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"]}],"source":["# Find the audio file in the output directory\n","\n","# Find all the audio files in the output directory\n","audio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n","\n","\n","# Select the first audio file in the list\n","audio_filename = audio_files[0]\n","\n","# Print the name of the selected audio file\n","print(audio_filename)"]},{"cell_type":"markdown","id":"a9fe2a4e-b6ac-43d3-9b22-7df437015913","metadata":{},"source":["## Task 3: Transcribe the Video using Whisper"]},{"cell_type":"markdown","id":"1a00b32c-06e2-4fb1-8830-3634b13d133a","metadata":{},"source":["In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n","\n","Using these variables we will:\n","- create a list to store the transcripts\n","- Read the Audio File \n","- Send the file to the Whisper Model using the OpenAI package "]},{"cell_type":"markdown","id":"e4b60c5a-ea58-469e-b699-d46ef1cc7485","metadata":{},"source":["To complete this step, create the following: \n","- A variable named `audio_file`that is assigned the `audio_filename` we created in the last step\n","- A variable named `output_file` that is assigned the value `\"files/transcripts/transcript.txt\"`\n","- A variable named `model` that is assigned the value  `\"whisper-1\"`\n","- An empty list called `transcripts`\n","- A variable named `audio` that uses the `open` method and `\"rb\"` modifier on the `audio_file`\n","- A variable to store the `response` from the `openai.Audio.transcribe` method that takes in the `model`and `audio` variables \n","- Append the `response[\"text\"]`to the `transcripts` list. "]},{"cell_type":"code","execution_count":5,"id":"54306dcc-40f7-4a12-97ef-388b95c70ad4","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":1667,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1695385362796,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define function parameters\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Print the name of the audio file\nprint(audio_file)\n\n# Transcribe the audio file to text using OpenAI API\nprint(\"converting audio to text...\")\nwith open(audio_file, \"rb\") as audio:\n    response = openai.Audio.transcribe(model, audio)\n\n# Extract the transcript from the response\ntranscript = (response[\"text\"])\n","outputsMetadata":{"0":{"height":76,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n","converting audio to text...\n"]},{"ename":"APIRemovedInV1","evalue":"\n\nYou tried to access openai.Audio, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverting audio to text...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(audio_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m audio:\n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Extract the transcript from the response\u001b[39;00m\n\u001b[1;32m     17\u001b[0m transcript \u001b[38;5;241m=\u001b[39m (response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n","\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Audio, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"]}],"source":["# Define function parameters\n","audio_file = audio_filename\n","output_file = \"files/transcripts/transcript.txt\"\n","model = \"whisper-1\"\n","\n","# Print the name of the audio file\n","print(audio_file)\n","\n","\n","# Transcribe the audio file to text using OpenAI API\n","print(\"converting audio to text...\")\n","\n","with open(audio_file, \"rb\") as audio:\n","    response = openai.Audio.transcribe(model, audio)\n","\n","# Extract the transcript from the response\n","transcript = (response[\"text\"])"]},{"cell_type":"markdown","id":"7255d301-69e2-4e04-813d-5a90b5ebcbdc","metadata":{},"source":["To save the transcripts to text files we will use the below provided code: "]},{"cell_type":"code","execution_count":null,"id":"6798bff4-ac8d-46e3-8c62-e540655e859d","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":28,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1695381457561,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# If an output file is specified, save the transcript to a .txt file\n\nif output_file is not None:\n    # Create the directory for the output file if it doesn't exist\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    # Write the transcript to the output file\n    with open(output_file, \"w\") as file:\n        file.write(transcript)\n\n# Print the transcript to the console to verify it worked \nprint(transcript)\n\n","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[],"source":["# If an output file is specified, save the transcript to a .txt file\n","\n","if output_file is not None:\n","    # Create the directory for the output file if it doesn't exist\n","    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n","    # Write the transcript to the output file\n","    with open(output_file, \"w\") as file:\n","        file.write(transcript)\n","\n","# Print the transcript to the console to verify it worked \n","print(transcript)\n","\n"]},{"cell_type":"markdown","id":"c1001454-eb29-4981-825f-fa08e2fc48e8","metadata":{},"source":["## Task 4: Create a TextLoader using LangChain "]},{"cell_type":"markdown","id":"8191715b-72ad-4a34-ac95-02e1fbf8d391","metadata":{},"source":["In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document. "]},{"cell_type":"markdown","id":"4f75f541-5bd7-4214-a75e-79681303c6f6","metadata":{},"source":["To complete this step, do the following: \n","- Import `TextLoader` from `langchain.document_loaders`\n","- Create a variable called `loader` that uses the `TextLoader` method which takes in the directory of the transcripts `\"./files/text\"`\n","- Create a variable called `docs` that is assigned the result of calling the `loader.load()` method. "]},{"cell_type":"code","execution_count":null,"id":"bb8654f7-965e-4e62-98ab-d08b7026e3d9","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1695381464265,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the TextLoader class from the langchain.document_loaders module\n\nfrom langchain.document_loaders import TextLoader\n\n# Create a new instance of the TextLoader class, specifying the directory containing the text files\n\nloader = TextLoader(\"./files/text\")\n\n# Load the documents from the specified directory using the TextLoader instance\n\ndocs = loader.load()"},"outputs":[],"source":["# Import the TextLoader class from the langchain.document_loaders module\n","\n","from langchain.document_loaders import TextLoader\n","\n","# Create a new instance of the TextLoader class, specifying the directory containing the text files\n","\n","loader = TextLoader(\"./files/text\")\n","\n","# Load the documents from the specified directory using the TextLoader instance\n","\n","docs = loader.load()"]},{"cell_type":"code","execution_count":null,"id":"269aaed5-7d07-43d7-a2d0-a89730ec4bc9","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":18,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1695381467004,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Show the first element of docs to verify it has been loaded \ndocs[0]"},"outputs":[],"source":["# Show the first element of docs to verify it has been loaded \n","docs[0]"]},{"cell_type":"markdown","id":"577069b3-02f6-4b73-aaaa-d8b8e8006d98","metadata":{},"source":["## Task 4: Creating an In-Memory Vector Store "]},{"cell_type":"markdown","id":"79af9e43-c32f-478d-b057-dc3b7890925e","metadata":{},"source":["Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n","\n","For large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n","\n","We will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. "]},{"cell_type":"markdown","id":"d3a5eb22-3a34-40a5-9f00-bd4895a1c4ca","metadata":{},"source":["### Instructions\n","\n","- Import the `tiktoken` package. "]},{"cell_type":"code","execution_count":null,"id":"15298bd3-5465-450d-b917-5e5d87d78bf2","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1695381479098,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the tiktoken package\nimport tiktoken"},"outputs":[],"source":["# Import the tiktoken package\n","import tiktoken"]},{"cell_type":"markdown","id":"6e01af9c-f5ea-4382-b7ff-01fe0bb30edc","metadata":{},"source":["## Task 5: Create the Document Search "]},{"cell_type":"markdown","id":"22438b44-b8f8-4c78-a573-87ee4bdb2234","metadata":{},"source":["We will now use LangChain to complete some important operations to create the Question and Answer experience. Let's import the following: \n","\n","- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n","- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n","- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n","- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n","- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. ("]},{"cell_type":"code","execution_count":null,"id":"3a7fb40d-de20-4ec8-b05a-036b6dc6ad66","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1695381482721,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the RetrievalQA class from the langchain.chains module\nfrom langchain.chains import RetrievalQA\n\n# Import the ChatOpenAI class from the langchain.chat_models module\nfrom langchain.chat_models import ChatOpenAI\n\n# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\nfrom langchain.vectorstores import DocArrayInMemorySearch\n\n# Import the OpenAIEmbeddings class from the langchain.embeddings module\nfrom langchain.embeddings import OpenAIEmbeddings"},"outputs":[],"source":["# Import the RetrievalQA class from the langchain.chains module\n","from langchain.chains import RetrievalQA\n","\n","# Import the ChatOpenAI class from the langchain.chat_models module\n","from langchain.chat_models import ChatOpenAI\n","\n","# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\n","from langchain.vectorstores import DocArrayInMemorySearch\n","\n","# Import the OpenAIEmbeddings class from the langchain.embeddings module\n","from langchain.embeddings import OpenAIEmbeddings"]},{"cell_type":"markdown","id":"9bec39d2-8a4c-4638-953f-fbd9fa47ad6f","metadata":{},"source":["Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. "]},{"cell_type":"markdown","id":"665d55d7-25fb-4aeb-9434-6b76de0ee405","metadata":{},"source":["To complete this step: \n","- Create a variable called `db`\n","- Assign the `db` variable to store the result of the method `DocArrayInMemorySearch.from_documents`\n","- In the DocArrayInMemorySearch method, pass in the `docs` and a function call to `OpenAIEmbeddings()`"]},{"cell_type":"code","execution_count":null,"id":"66ef212c-eefd-4cf2-a02c-3c01b1b29118","metadata":{"executionCancelledAt":null,"executionTime":1041,"lastExecutedAt":1695381624056,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\ndb = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)"},"outputs":[],"source":["# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\n","db = DocArrayInMemorySearch.from_documents(\n","    docs, \n","    OpenAIEmbeddings()\n",")"]},{"cell_type":"markdown","id":"033f3ebc-c098-49e8-a96f-f428940996d9","metadata":{},"source":["We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM."]},{"cell_type":"markdown","id":"0aabff95-8fa2-47ea-b0b3-23c53c6d3c38","metadata":{},"source":["Create the following: \n","- A variable called `retriever` that is assigned `db.as_retriever()`\n","- A variable called `llm` that creates the `ChatOpenAI` method with a set `temperature`of `0.0`. This will controle the variability in the responses we receive from the LLM. "]},{"cell_type":"code","execution_count":null,"id":"7c7f6113-c145-47ff-ab9c-ada04ca047ce","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1695381626978,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Convert the DocArrayInMemorySearch instance to a retriever\nretriever = db.as_retriever()\n\n# Create a new ChatOpenAI instance with a temperature of 0.0\nllm = ChatOpenAI(temperature = 0.0)"},"outputs":[],"source":["# Convert the DocArrayInMemorySearch instance to a retriever\n","retriever = db.as_retriever()\n","\n","# Create a new ChatOpenAI instance with a temperature of 0.0\n","llm = ChatOpenAI(temperature = 0.0)"]},{"cell_type":"markdown","id":"4f5a7c7a-1676-41e0-976a-50316a684d12","metadata":{},"source":["Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n","- The `llm` we want to use\n","- The `chain_type` which is how the model retrieves the data\n","- The `retriever` that we have created \n","- An option called `verbose` that allows use to see the seperate steps of the chain "]},{"cell_type":"markdown","id":"2a5ce4f9-e025-40e6-b737-be77213d5110","metadata":{},"source":["Create a variable called `qa_stuff`. This variable will be assigned the method `RetrievalQA.from_chain_type`. \n","\n","Use the following settings inside this method: \n","- `llm=llm`\n","- `chain_type=\"stuff\"`\n","- `retriever=retriever`\n","- `verbose=True`"]},{"cell_type":"code","execution_count":null,"id":"09fc202b-198f-4510-8d81-258f914d5c08","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1695381629777,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a new RetrievalQA instance with the specified parameters\nqa_stuff = RetrievalQA.from_chain_type(\n    llm=llm,            # The ChatOpenAI instance to use for generating responses\n    chain_type=\"stuff\", # The type of chain to use for the QA system\n    retriever=retriever, # The retriever to use for retrieving relevant documents\n    verbose=True        # Whether to print verbose output during retrieval and generation\n)"},"outputs":[],"source":["# Create a new RetrievalQA instance with the specified parameters\n","qa_stuff = RetrievalQA.from_chain_type(\n","    llm=llm,            # The ChatOpenAI instance to use for generating responses\n","    chain_type=\"stuff\", # The type of chain to use for the QA system\n","    retriever=retriever, # The retriever to use for retrieving relevant documents\n","    verbose=True        # Whether to print verbose output during retrieval and generation\n",")"]},{"cell_type":"markdown","id":"4ce8cff5-ab49-44f0-96ee-88826d88ea6a","metadata":{},"source":["## Task 5: Create the Queries "]},{"cell_type":"markdown","id":"d51218d4-4e81-4d87-9f3f-77eacde057c1","metadata":{},"source":["Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. "]},{"cell_type":"markdown","id":"5e2b036b-cef6-4b52-9421-5ccf2b865482","metadata":{},"source":["To create the questions to ask the model complete the following steps: \n","- Create a variable call `query` and assigned it a string value of `\"What is this tutorial about?\"`\n","- Create a `response` variable that will store the result of `qa_stuff.run(query)` \n","- Show the `resposnse`"]},{"cell_type":"code","execution_count":null,"id":"d576672c-5078-487a-9dc5-3703f17d82f1","metadata":{"executionCancelledAt":null,"executionTime":4949,"lastExecutedAt":1695381641995,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Set the query to be used for the QA system\nquery = \"What is this tutorial about?\"\n\n# Run the query through the RetrievalQA instance and store the response\nresponse = qa_stuff.run(query)\n\n# Print the response to the console\nresponse\n","outputsMetadata":{"0":{"height":76,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query = \"What is this tutorial about?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","response = qa_stuff.run(query)\n","\n","# Print the response to the console\n","response\n"]},{"cell_type":"markdown","id":"de9f2df3-87d1-40d3-862a-95769c11d015","metadata":{},"source":["We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. "]},{"cell_type":"code","execution_count":null,"id":"dbb75225-76c1-4eb8-9055-e13a3bb68682","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":117,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query = \"What is the difference between a training set and test set?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","response = qa_stuff.run(query)\n","\n","# Print the response to the console\n","response"]},{"cell_type":"code","execution_count":null,"id":"13864a14-0eda-4afd-bfe5-90fdefbc5d49","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":117,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query = \"Who should watch this lesson?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","response = qa_stuff.run(query)\n","\n","# Print the response to the console\n","response "]},{"cell_type":"code","execution_count":null,"id":"c62b73e4-e746-49f9-8106-921cbb4e6df8","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":117,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query =\"Who is the greatest football team on earth?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","response = qa_stuff.run(query)\n","\n","# Print the response to the console\n","response "]},{"cell_type":"code","execution_count":null,"id":"f9f7a7f3-f0f1-44ad-be76-d15aa009ac34","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":117,"type":"stream"},"1":{"height":77,"type":"stream"}}},"outputs":[],"source":["# Set the query to be used for the QA system\n","query = \"How long is the circumference of the earth?\"\n","\n","# Run the query through the RetrievalQA instance and store the response\n","response = qa_stuff.run(query)\n","\n","# Print the response to the console\n","response "]},{"cell_type":"markdown","id":"65454beb-970f-4af6-a04c-798b9f665b6f","metadata":{},"source":["## All done, congrats! "]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
